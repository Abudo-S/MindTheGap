{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mind the gap\n",
        "\n",
        "This project aims to identify, measure, and mitigate social biases, such as gender, race, or profession-related stereotypes, in lightweight transformer models through hands-on fine-tuning and evaluation on targeted NLP tasks. More specifically, the project should implement a four-step methodology, defined as follows:\n",
        "\n",
        "1. Choose a lightweight pre-trained transformer model (e.g., DistilBERT, ALBERT, RoBERTa-base) suitable for local fine-tuning and evaluation.\n",
        "2. Evaluate the presence and extent of social bias (e.g., gender, racial, or occupational stereotypes) using dedicated benchmark datasets. Both quantitative metrics and qualitative outputs should be evaluated.\n",
        "3. Apply a bias mitigation technique, such as **fine-tuning on curated counter-stereotypical data**, integrating **adapter layers**, or employing **contrastive learning**, while keeping the solution computationally efficient and transparent.\n",
        "4. Re-assess the model using the same benchmark(s) to measure improvements. Students should compare pre- and post-intervention results, discuss trade-offs (e.g., performance vs. fairness), and visualize the impact of their approach.\n",
        "\n",
        "#### Dataset\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://github.com/moinnadeem/StereoSet). Nadeem, M., Bethke, A., & Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. *arXiv preprint arXiv:2004.09456*.\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://huggingface.co/datasets/McGill-NLP/stereoset). Hugging Face co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZodFEpEh5vD",
        "outputId": "064af7a7-d811-43dc-c382-45fd8929557e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abudo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import json\n",
        "#to output plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import importlib\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# %cd /content/\n",
        "# !ls -R\n",
        "# repo_name = \"MindTheGap\"\n",
        "# #!rm -rf {repo_name}\n",
        "# if not os.path.exists(repo_name):\n",
        "#     print(f\"Directory {repo_name} does not exist, proceeding with clone.\")\n",
        "#     !git clone https://github.com/Abudo-S/MindTheGap.git\n",
        "\n",
        "# %cd MindTheGap\n",
        "\n",
        "import AdaptedMLMTransformer as AdaptedMLMTransformer_Module\n",
        "import AdaptedNSPTransformer as AdaptedNSPTransformer_Module\n",
        "import intersentence_loader as intersentence_loader_Module\n",
        "import dataloader\n",
        "importlib.reload(AdaptedMLMTransformer_Module) # in case of updates\n",
        "importlib.reload(AdaptedNSPTransformer_Module) # in case of updates\n",
        "importlib.reload(intersentence_loader_Module) # in case of updates\n",
        "importlib.reload(dataloader) # in case of updates\n",
        "from AdaptedMLMTransformer import AdaptedMLMTransformer\n",
        "from AdaptedNSPTransformer import AdaptedNSPTransformer\n",
        "from intersentence_loader import IntersentenceDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General observations:\n",
        "- The given dataset contains 2123 intersentence contexts and 2106 intrasentence contexts, each context has 3 sentences that are labeled as ```stereotype, anti-stereotype and unrelated``` with respect to the target bias type.\n",
        "- Intrasentences focus on biases at the word or sub-phrase level at position \"BLANK\". Meanwhile intersentences focus on biases within the relationship between the context and its associated sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tasks:\n",
        "- Intrasentences are considered as a masked-language-modeling (MLM) task in which the model is given a sentence where a certain percentage of the tokens have been replaced with a special [MASK] token. The model's task is to predict the original words/sub-words of the masked tokens based on the surrounding context.\n",
        "- Intersentences are considered as a next-sentence-prediction (NSP) task in which the model is given a pair of sentences, a (context) and (one of the associated sentences) formated as ```[CLS] sentence_A [SEP] sentence_B [SEP]```. Then the model outputs the probabilities that describe the relationship between both sentences into one of three categories:\n",
        "\n",
        "    * IsNext [1]: The second sentence is the actual next sentence in the original text corpus, following the first.\n",
        "\n",
        "    * NotNext [0]: The second sentence is unrelated to the first.\n",
        "\n",
        "The probability for the \"IsNext\" class indicates how confident the model is that the second sentence logically follows the first. In our case we can use the probability of \"IsNext\" to compare the conjunction between each associated labeled sentence with the main context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pLn3HuBpdezQ"
      },
      "outputs": [],
      "source": [
        "# original_init = AdaptedMLMTransformer_Module.AdaptedMLMTransformer.__init__\n",
        "\n",
        "# # Corrected debug_init signature - assuming only model_name is a required parameter\n",
        "# def debug_init(self, model_name=\"roberta-base\"):\n",
        "#     print(f\"Initializing AdaptedMLMTransformer with model_name: {model_name}\")\n",
        "#     # Pass only the parameters that original_init expects\n",
        "#     original_init(self, model_name=model_name)\n",
        "#     print(\"AdaptedMLMTransformer initialized.\")\n",
        "#     for name, param in self.named_parameters():\n",
        "#         print(f\"Layer: {name}, Device: {param.device}, Dtype: {param.dtype}\")\n",
        "\n",
        "# AdaptedMLMTransformer_Module.LightTAdaptedMLMTransformerransformerModel.__init__ = debug_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YEZqNR5tnUOv",
        "outputId": "c48924c1-6de3-4e7a-a5ab-cbd169904b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data/stereo_dataset.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "Underlying model moved to cuda.\n",
            "AdaptedNSPTransformer instance moved to GPU.\n",
            "Number of parameters: 124,647,170\n",
            "Let's use 1 GPUs!\n",
            "Maximum sequence length found: -inf\n",
            "Calculating intersentences prediction...\n",
            "Maximum sequence length found: -inf\n",
            "Calculating intersentences prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1274/1274 [01:18<00:00, 16.30it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using pretrained class: roberta-base\n",
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "RobertaForMaskedLM instance moved to GPU.\n",
            "\n",
            "RobertaForMaskedLM instance moved to GPU.\n",
            "\n",
            "Calculating intrasentences prediction...\n",
            "Calculating intrasentences prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2204/2204 [03:15<00:00, 11.30it/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "# os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
        "INPUT_FILE = \"data/stereo_dataset.json\" #\"/content/MindTheGap/data/stereo_dataset.json\"\n",
        "OUTPUT_FILE_PRETRAINED =\"predictions.json\"\n",
        "OUTPUT_FILE_FINETUNED =\"predictions.json\"\n",
        "OUTPUT_DIR = \"predictions/\"\n",
        "BATCH_SIZE = 5\n",
        "MAX_SEQ_LENGTH = 128\n",
        "NO_CUDA = False\n",
        "\n",
        "class BiasEvaluator():\n",
        "    def __init__(self,\n",
        "                 input_file=\"data/stereo_dataset.json\",\n",
        "                 model_name=\"roberta-base\",\n",
        "                 intersentence_load_path=None, \n",
        "                 intrasentence_load_path=None, \n",
        "                 skip_intrasentence=False,\n",
        "                 skip_intersentence=False):\n",
        "        print(f\"Loading {input_file}...\")\n",
        "\n",
        "        #self.dataloader = dataloader.StereoSet(os.path.abspath(input_file))\n",
        "        self.input_file = input_file\n",
        "        self.model_name = model_name\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "        self.SKIP_INTERSENTENCE = skip_intersentence\n",
        "        self.SKIP_INTRASENTENCE = skip_intrasentence\n",
        "        self.tokenizer = AdaptedMLMTransformer(model_name=self.model_name).tokenizer\n",
        "        self.device = \"cuda\" if not NO_CUDA else \"cpu\"\n",
        "\n",
        "        # to keep padding consistent with the other models -> improves LM score.\n",
        "        if self.tokenizer.__class__.__name__ == \"XLNetTokenizer\":\n",
        "            self.tokenizer.padding_side = \"right\"\n",
        "        self.MASK_TOKEN = self.tokenizer.mask_token\n",
        "\n",
        "        # Set this to be none if you don't want to batch items together!\n",
        "        self.max_seq_length = None if BATCH_SIZE == 1 else MAX_SEQ_LENGTH\n",
        "\n",
        "        self.MASK_TOKEN_IDX = self.tokenizer.encode(\n",
        "            self.MASK_TOKEN, add_special_tokens=False)\n",
        "        assert len(self.MASK_TOKEN_IDX) == 1\n",
        "        self.MASK_TOKEN_IDX = self.MASK_TOKEN_IDX[0]\n",
        "\n",
        "    def evaluate_intrasentence(self):\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"Using pretrained class: {self.model_name}\")\n",
        "\n",
        "        model = AdaptedMLMTransformer(model_name=self.model_name).model\n",
        "\n",
        "        config = AutoConfig.from_pretrained(self.model_name)\n",
        "\n",
        "        print(f\"Model max position embeddings: {config.max_position_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                #model.tokenizer.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "            print(f\"{model.__class__.__name__} instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        if torch.cuda.device_count() > 1 and self.device == \"cuda\":\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model)\n",
        "        model.eval()\n",
        "\n",
        "        print()\n",
        "        if self.INTRASENTENCE_LOAD_PATH:\n",
        "            state_dict = torch.load(self.INTRASENTENCE_LOAD_PATH)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        #pad_to_max_length = True if BATCH_SIZE > 1 else False\n",
        "\n",
        "        #11017 intrasentences\n",
        "        dataset = dataloader.IntrasentenceLoader(self.tokenizer, max_seq_length=self.max_seq_length,\n",
        "                                                 pad_to_max_length='max_length',\n",
        "                                                 input_file= self.input_file)\n",
        "        \n",
        "        #reproducible split of dataset into train and test sets\n",
        "        # gen = torch.Generator().manual_seed(41)\n",
        "        # training_size = int(0.85 * len(dataset))\n",
        "        # test_size = len(dataset) - training_size\n",
        "        # train_dataset, test_dataset = random_split(dataset, [training_size, test_size], generator=gen)\n",
        "        # print(f\"First element of the [intrasentences] training set: {train_dataset[0]}\")\n",
        "        # print(f\"First element of the [intrasentances] test set: {test_dataset[0]}\")  \n",
        "\n",
        "        #print(f'Sentence 0: {dataset.sentences[0]}')\n",
        "        data_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "        word_probabilities = defaultdict(list)\n",
        "\n",
        "        print(\"Calculating intrasentences prediction...\")\n",
        "        \n",
        "        # calculate the logits for each prediction\n",
        "        for sentence_id, next_token, input_ids, attention_mask, token_type_ids in tqdm(data_loader, total=len(data_loader)):\n",
        "            # start by converting everything to a tensor\n",
        "            if BATCH_SIZE == 1:\n",
        "              print(f\"Max attention mask value: {max(attention_mask)}\")\n",
        "              print(f\"Min attention mask value: {min(attention_mask)}\")\n",
        "              max_id = max(input_ids)\n",
        "              print(f\"Max input ID: {max_id}, Model vocab size: {self.tokenizer.vocab_size}\")\n",
        "\n",
        "            input_ids = input_ids.squeeze(1).to(self.device)\n",
        "            attention_mask = attention_mask.squeeze(1).to(self.device)\n",
        "            next_token = next_token.to(self.device) #token to predict\n",
        "            # else:  #in case of batch_size > 1\n",
        "            #   print(input_ids)\n",
        "            #   input_ids = torch.stack(input_ids).to(self.device).transpose(0, 1)\n",
        "            #   attention_mask = torch.stack(attention_mask).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "            #   next_token = next_token.to(self.device)\n",
        "            #   token_type_ids = torch.stack(token_type_ids).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "\n",
        "            mask_idxs = (input_ids == self.MASK_TOKEN_IDX)\n",
        "\n",
        "            # get the probabilities\n",
        "            output = model(input_ids, attention_mask=attention_mask)[0].softmax(dim=-1)\n",
        "\n",
        "            output = output[mask_idxs] #target only the masked positions\n",
        "            output = output.index_select(1, next_token).diag() #extract the probs of true tokens from the vocabulary dimension\n",
        "            for idx, item in enumerate(output):\n",
        "                word_probabilities[sentence_id[idx]].append(item.item())\n",
        "\n",
        "        # now reconcile the probabilities into sentences\n",
        "        sentence_probabilties = []\n",
        "        for k, v in word_probabilities.items():\n",
        "            pred = {}\n",
        "            pred['id'] = k\n",
        "            #since we have n next tokens for the same sentance id, associated probs needs to be standarized\n",
        "            #in order to be compared with other labeled sentences' scores\n",
        "            # score = np.sum([np.log2(i) for i in v]) + np.log2(len(v))\n",
        "            score = np.mean(v) \n",
        "            pred['score'] = score\n",
        "\n",
        "            sentence_probabilties.append(pred)\n",
        "\n",
        "        return sentence_probabilties\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    def evaluate_intersentence(self):\n",
        "        model = AdaptedNSPTransformer().to(self.device)\n",
        "\n",
        "        config = AutoConfig.from_pretrained(self.model_name)\n",
        "\n",
        "        print(f\"Model max position embeddings: {config.max_position_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                #model.tokenizer.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "            print(f\"{model.__class__.__name__} instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        print(f\"Number of parameters: {self.count_parameters(model):,}\")\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "        if self.INTERSENTENCE_LOAD_PATH:\n",
        "            model.load_state_dict(torch.load(self.INTERSENTENCE_LOAD_PATH))\n",
        "\n",
        "        model.eval()\n",
        "        dataset = IntersentenceDataset(self.tokenizer)\n",
        "        \n",
        "        #reproducible split of dataset into train and test sets\n",
        "        # gen = torch.Generator().manual_seed(42)\n",
        "        # training_size = int(0.85 * len(dataset))\n",
        "        # test_size = len(dataset) - training_size\n",
        "        # train_dataset, test_dataset = random_split(dataset, [training_size, test_size], generator=gen)\n",
        "        # print(f\"First element of the [intersentences] training set: {train_dataset[0]}\")\n",
        "        # print(f\"First element of the [intersentances] test set: {test_dataset[0]}\")  \n",
        "\n",
        "        #print(f'Sentence 0: {dataset.sentences[0]}')\n",
        "        data_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        print(\"Calculating intersentences prediction...\")\n",
        "        if NO_CUDA:\n",
        "            n_cpus = cpu_count()\n",
        "            print(f\"Using {n_cpus} cpus!\")\n",
        "            predictions = Parallel(n_jobs=n_cpus, backend=\"multiprocessing\")(delayed(process_job)(\n",
        "                batch, model, self.model_name) for batch in tqdm(data_loader, total=len(data_loader)))\n",
        "        else:\n",
        "            predictions = []\n",
        "\n",
        "            for batch_num, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "                input_ids, token_type_ids, attention_mask, sentence_id = batch\n",
        "                input_ids = input_ids.squeeze(1).to(self.device)\n",
        "                attention_mask = attention_mask.squeeze(1).to(self.device)\n",
        "                #token_type_ids = token_type_ids.to(self.device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                if hasattr(outputs, \"logits\"):\n",
        "                    outputs = outputs.logits\n",
        "                else:\n",
        "                    outputs = outputs[0]\n",
        "                outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                for idx in range(input_ids.shape[0]):\n",
        "                    probabilities = {}\n",
        "                    probabilities['id'] = sentence_id[idx]\n",
        "                    #probability of the second sentence to be \"next\" to the first one, [idx, 1] corresponds to the positive class \n",
        "                    probabilities['score'] = outputs[idx, 1].item()\n",
        "                    predictions.append(probabilities)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self):\n",
        "        bias = {}\n",
        "        if not self.SKIP_INTERSENTENCE:\n",
        "            intersentence_bias = self.evaluate_intersentence()\n",
        "            bias['intersentence'] = intersentence_bias\n",
        "\n",
        "        if not self.SKIP_INTRASENTENCE:\n",
        "            intrasentence_bias = self.evaluate_intrasentence()\n",
        "            bias['intrasentence'] = intrasentence_bias\n",
        "        return bias\n",
        "\n",
        "\n",
        "def process_job(batch, model, pretrained_class):\n",
        "    input_ids, token_type_ids, sentence_id = batch\n",
        "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "    if type(outputs) == tuple:\n",
        "        outputs = outputs[0]\n",
        "    outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    pid = sentence_id[0]\n",
        "    #probability of the second sentence to be \"next\" to the first one, [idx, 1] corresponds to the positive class \n",
        "    pscore = outputs[0, 1].item()\n",
        "    return (pid, pscore)\n",
        "\n",
        "\n",
        "pretrained_model_name=\"roberta-base\"\n",
        "evaluator = BiasEvaluator(input_file=INPUT_FILE, model_name=pretrained_model_name)\n",
        "results = evaluator.evaluate()\n",
        "results\n",
        "\n",
        "output_file = f\"{pretrained_model_name}_{OUTPUT_FILE_PRETRAINED}\"\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, output_file)\n",
        "with open(output_file, \"w+\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
