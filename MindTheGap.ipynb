{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emv880Hk4hAq"
      },
      "source": [
        "### Mind the gap\n",
        "\n",
        "This project aims to identify, measure, and mitigate social biases, such as gender, race, or profession-related stereotypes, in lightweight transformer models through hands-on fine-tuning and evaluation on targeted NLP tasks. More specifically, the project should implement a four-step methodology, defined as follows:\n",
        "\n",
        "1. Choose a lightweight pre-trained transformer model (e.g., DistilBERT, ALBERT, RoBERTa-base) suitable for local fine-tuning and evaluation.\n",
        "2. Evaluate the presence and extent of social bias (e.g., gender, racial, or occupational stereotypes) using dedicated benchmark datasets. Both quantitative metrics and qualitative outputs should be evaluated.\n",
        "3. Apply a bias mitigation technique, such as **fine-tuning on curated counter-stereotypical data**, integrating **adapter layers**, or employing **contrastive learning**, while keeping the solution computationally efficient and transparent.\n",
        "4. Re-assess the model using the same benchmark(s) to measure improvements. Students should compare pre- and post-intervention results, discuss trade-offs (e.g., performance vs. fairness), and visualize the impact of their approach.\n",
        "\n",
        "#### Dataset\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://github.com/moinnadeem/StereoSet). Nadeem, M., Bethke, A., & Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. *arXiv preprint arXiv:2004.09456*.\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://huggingface.co/datasets/McGill-NLP/stereoset). Hugging Face co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZodFEpEh5vD",
        "outputId": "1ef5e5fa-41c4-4e14-b903-f1f8ea386329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 2] The system cannot find the file specified: 'MindTheGap'\n",
            "c:\\Users\\abudo\\source\\vscode_projects\\MindTheGap\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import json\n",
        "#to output plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import importlib\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# %cd /content/\n",
        "# !ls -R\n",
        "# repo_name = \"MindTheGap\"\n",
        "# #!rm -rf {repo_name}\n",
        "# if not os.path.exists(repo_name):\n",
        "#     print(f\"Directory {repo_name} does not exist, proceeding with clone.\")\n",
        "#     !git clone https://github.com/Abudo-S/MindTheGap.git\n",
        "\n",
        "%cd MindTheGap\n",
        "\n",
        "import AdaptedMLMTransformer as AdaptedMLMTransformer_Module\n",
        "import AdaptedNSPTransformer as AdaptedNSPTransformer_Module\n",
        "import intersentence_loader as intersentence_loader_Module\n",
        "import dataloader\n",
        "importlib.reload(AdaptedMLMTransformer_Module) # in case of updates\n",
        "importlib.reload(AdaptedNSPTransformer_Module) # in case of updates\n",
        "importlib.reload(intersentence_loader_Module) # in case of updates\n",
        "importlib.reload(dataloader) # in case of updates\n",
        "from AdaptedMLMTransformer import AdaptedMLMTransformer\n",
        "from AdaptedNSPTransformer import AdaptedNSPTransformer\n",
        "from intersentence_loader import IntersentenceDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoNBBQ5D4hAw"
      },
      "source": [
        "### General observations:\n",
        "- The given dataset contains 2123 intersentence contexts and 2106 intrasentence contexts, each context has 3 sentences that are labeled as ```stereotype, anti-stereotype and unrelated``` with respect to the target bias type.\n",
        "- Intrasentences focus on biases at the word or sub-phrase level at position \"BLANK\". Meanwhile intersentences focus on biases within the relationship between the context and its associated sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7B4AW564hAw"
      },
      "source": [
        "#### Tasks:\n",
        "- Intrasentences are considered as a masked-language-modeling (MLM) task in which the model is given a sentence where a certain percentage of the tokens have been replaced with a special [MASK] token. The model's task is to predict the original words/sub-words of the masked tokens based on the surrounding context.\n",
        "- Intersentences are considered as a next-sentence-prediction (NSP) task in which the model is given a pair of sentences, a (context) and (one of the associated sentences) formated as ```[SEP] sentence_A [SEP] sentence_B [SEP]``` or ```[CLS] sentence_A [SEP] sentence_B [SEP]```. Then the model outputs the probabilities that describe the relationship between both sentences into one of three categories:\n",
        "\n",
        "    * IsNext [1]: The second sentence is the actual next sentence in the original text corpus, following the first.\n",
        "\n",
        "    * NotNext [0]: The second sentence is unrelated to the first.\n",
        "\n",
        "The probability for the \"IsNext\" class indicates how confident the model is that the second sentence logically follows the first. In our case we can use the probability of \"IsNext\" to compare the conjunction between each associated labeled sentence with the main context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7sVvS2u6OY9"
      },
      "source": [
        "## Data Preprocessing\n",
        "- Intrasentences: The dataloader creates a sentence for each target token of the masked word/subword that replaces \"BLANK\" with respect to the target word, concatenating the original context.\n",
        "\n",
        "- Intersentences: The dataloader creates a combined special sentence, concatenating the context with each associated sentence singularly. The generated sentence depends on the used tokenizer, some tokenizers use [CLS] and [SEP], others use only [SEP]. So it's necessary to use a compatible tokenzier with the pre-trained model.\n",
        "\n",
        "\n",
        "#### Context's sentences Evaluation\n",
        "- Intrasentences: The MLM model should predict the masked token in each sentence, calculating per each context the mean score of target tokens w.r.t. model's vocabulary. So our task isn't retrieving the highest score belonging to a random predicted word, but retrieving the score of our target word's tokens. Later the score belonging to each sentence is compared to the scores of other sentences belonging to the same context, in order to determine model's preference w.r.t. the target bias.\n",
        "\n",
        "- Intersentences: The NSP model should predict whether the second sentence can be the actual next sentence in the original text or not. So our task is to retrieve the score that describes the probability of the second sentence to follow the first sentence.\n",
        "\n",
        "\n",
        "#### Loss calculation\n",
        "Applying Mean Squared Error (MSE) loss with the following target labels in case of different tasks.\n",
        "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "- Intrasentences: The loss is calculated by taking average loss between the output tokens' probabilities and different token-score thresholds based on sentence's label:  \n",
        "    * Stereotyped sentences: the minmum value of the softMax function (theoretically 0 but since the softmax never outputs zero in practical evaluation, so we decided to set a very small threshold = **1e-5** for stereotyped sentences).\n",
        "    * Anti-stereotyped sentences: the maximum value of the softMax function (theoretically 1 but since the softmax never outputs 1 in practical evaluation, so we decided to set a very small threshold =  **0.999** for anti-stereotyped sentences).\n",
        "    * Unrelated sentences: w'd consider a threshold = **0.50** for neutral/unrelated sentences since they less relevant to the main context.\n",
        "\n",
        "- Intersentences: The loss is calculated by comparing the predict score of the second sentence (nextScore) and the pre-determined thresholds based on sentence's labels:  \n",
        "    * Stereotyped sentences: Since we'd like to reduce the prediction scores of stereotyped sentences (sentences labeled as \"stereotype\"); we can label them with **1e-5** (Not 0 for the same reason in the case of intrasentences), so when the model gives a reasonable score for a stereotyped sentence as a next sentence (nextScore > 0), we still need to **minimize** that score through an optimization process.\n",
        "    * Anti-stereotyped sentences: viceversa for anti-stereotyped sentences; we can label them with **0.999** (Not 1 for the same reason in the case of intrasentences), so when the model gives a reasonable score for an anti-stereotyped sentence as a next sentence (nextScore < 1), we still need to **maximize** that score through an optimization procedure.  \n",
        "    * Unrelated sentences: Meanwhile for unrelated sentences; we can label them with **0.50**, so if the model gives a a reasonable score for an unrelated sentence as a next sentence (nextScore > 0.50), we'd need to minimize the score w.r.t. the threshold of neutral/unrelated sentences (nextScore = 0.50).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfT4tQn61ePM"
      },
      "source": [
        "### Data Split and Model Comparability\n",
        "--todo--  \n",
        "- train NSP head of AutoModelForSequenceClassification over training set (freeze pre-trained params except those beloning to the NSP head).\n",
        "- explain test data on pre-trained model (loss and SS evaluation in MLM and NSP) with bar plots.\n",
        "- explain training data on fine-tuned adapted model (loss and SS evaluation over epochs in MLM and NSP) with curve plots. (freeze pre-trained params including those belonging NSP head).\n",
        "- add explainability for bad scores either in case of pre-trained or fine-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pLn3HuBpdezQ"
      },
      "outputs": [],
      "source": [
        "# original_init = AdaptedMLMTransformer_Module.AdaptedMLMTransformer.__init__\n",
        "\n",
        "# # Corrected debug_init signature - assuming only model_name is a required parameter\n",
        "# def debug_init(self, model_name=\"roberta-base\"):\n",
        "#     print(f\"Initializing AdaptedMLMTransformer with model_name: {model_name}\")\n",
        "#     # Pass only the parameters that original_init expects\n",
        "#     original_init(self, model_name=model_name)\n",
        "#     print(\"AdaptedMLMTransformer initialized.\")\n",
        "#     for name, param in self.named_parameters():\n",
        "#         print(f\"Layer: {name}, Device: {param.device}, Dtype: {param.dtype}\")\n",
        "\n",
        "# AdaptedMLMTransformer_Module.LightTAdaptedMLMTransformerransformerModel.__init__ = debug_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YEZqNR5tnUOv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data/stereo_dataset.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "Underlying model moved to cuda.\n",
            "AdaptedNSPTransformer instance moved to GPU.\n",
            "Number of parameters: 124,647,170\n",
            "Let's use 1 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abudo\\source\\vscode_projects\\MindTheGap\\intersentence_loader.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if (len(sentence_tokens) + len(context_tokens)) > self.emp_max_seq_length:\n",
            "c:\\Users\\abudo\\source\\vscode_projects\\MindTheGap\\intersentence_loader.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  while (len(sentence_tokens) + len(context_tokens)) < self.max_seq_length:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length found: -inf\n",
            "First element of the [intersentences] training set: (tensor([[    0, 13584,  1623,  1364,    25,    10,   475,  2137,     4,     2,\n",
            "             2, 22649, 31720,    32,   747, 11216,     4,     2,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), '8f4fda0da82ef660dc2b5a315cccf8be', 0.5)\n",
            "First element of the [intersentances] test set: (tensor([[    0,   133, 22629,  5508,    10,   333,     9, 10717,     4,     2,\n",
            "             2,  1213, 33767, 17867,   859,  9412,    23,   106,     8, 33615,\n",
            "           553,   106,   141,    49,   360,    58,   164,     4,     2,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'c56fb293d77f807579f0ffbd6a37ae42', 1e-05)\n",
            "Calculating intersentence predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/192 [00:00<?, ?it/s]C:\\Users\\abudo\\AppData\\Local\\Temp\\ipykernel_13184\\2550515139.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = self.loss_fn(torch.tensor(outputs[idx, 1].item()), torch.tensor(sentence_label[idx]))\n",
            "  1%|          | 1/192 [00:00<01:03,  3.02it/s]C:\\Users\\abudo\\AppData\\Local\\Temp\\ipykernel_13184\\2550515139.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = self.loss_fn(torch.tensor(outputs[idx, 1].item()), torch.tensor(sentence_label[idx]))\n",
            "100%|██████████| 192/192 [00:12<00:00, 15.60it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using pretrained class: roberta-base\n",
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "RobertaForMaskedLM instance moved to GPU.\n",
            "\n",
            "RobertaForMaskedLM instance moved to GPU.\n",
            "\n",
            "First element of the [intrasentences] training set: ('f8eadc4d1b9a7e3d8f8c045c6c90a1e6', 10733, tensor([[    0,  2895, 27775,    82,    32,   543, 50264,     4,     2,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), [], 0.999)\n",
            "First element of the [intrasentances] test set: ('13c71caa1e8605db774b62197921f827', 41009, tensor([[    0,   133,  5051,  2269, 50264,    32,   684,   198,     5,   232,\n",
            "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), [], 0.999)\n",
            "Calculating intrasentence predictions...\n",
            "First element of the [intrasentences] training set: ('f8eadc4d1b9a7e3d8f8c045c6c90a1e6', 10733, tensor([[    0,  2895, 27775,    82,    32,   543, 50264,     4,     2,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), [], 0.999)\n",
            "First element of the [intrasentances] test set: ('13c71caa1e8605db774b62197921f827', 41009, tensor([[    0,   133,  5051,  2269, 50264,    32,   684,   198,     5,   232,\n",
            "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), [], 0.999)\n",
            "Calculating intrasentence predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331/331 [00:29<00:00, 11.09it/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "# os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
        "INPUT_FILE = \"data/stereo_dataset.json\"\n",
        "#INPUT_FILE = \"/content/MindTheGap/data/stereo_dataset.json\"\n",
        "OUTPUT_FILE_PRETRAINED =\"predictions.json\"\n",
        "OUTPUT_FILE_FINETUNED =\"predictions.json\"\n",
        "OUTPUT_DIR = \"predictions/\"\n",
        "BATCH_SIZE = 5\n",
        "MAX_SEQ_LENGTH = 128\n",
        "NO_CUDA = False\n",
        "TRAINING_SET_SIZE_PERCENT = 0.85\n",
        "\n",
        "class SentenceEvaluator():\n",
        "    def __init__(self,\n",
        "                 input_file=\"data/stereo_dataset.json\",\n",
        "                 model_name=\"roberta-base\",\n",
        "                 intersentence_load_path=None,\n",
        "                 intrasentence_load_path=None,\n",
        "                 skip_intrasentence=False,\n",
        "                 skip_intersentence=False,\n",
        "                 loss_fn = nn.MSELoss):\n",
        "        print(f\"Loading {input_file}...\")\n",
        "\n",
        "        #self.dataloader = dataloader.StereoSet(os.path.abspath(input_file))\n",
        "        self.input_file = input_file\n",
        "        self.model_name = model_name\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "        self.SKIP_INTERSENTENCE = skip_intersentence\n",
        "        self.SKIP_INTRASENTENCE = skip_intrasentence\n",
        "        self.loss_fn = loss_fn()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.device = \"cuda\" if not NO_CUDA else \"cpu\"\n",
        "        self.MASK_TOKEN = self.tokenizer.mask_token\n",
        "        #saved after first dat split\n",
        "        self.intra_train_dataset = None\n",
        "        self.intra_test_dataset = None\n",
        "        self.inter_train_dataset = None\n",
        "        self.inter_test_dataset = None\n",
        "\n",
        "        # Set this to be none if you don't want to batch items together!\n",
        "        self.max_seq_length = None if BATCH_SIZE == 1 else MAX_SEQ_LENGTH\n",
        "\n",
        "        self.MASK_TOKEN_IDX = self.tokenizer.encode(\n",
        "            self.MASK_TOKEN, add_special_tokens=False)\n",
        "        assert len(self.MASK_TOKEN_IDX) == 1\n",
        "        self.MASK_TOKEN_IDX = self.MASK_TOKEN_IDX[0]\n",
        "\n",
        "    def evaluate_intrasentence(self, useAdapter=False):\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"Using pretrained class: {self.model_name}\")\n",
        "\n",
        "        model = AdaptedMLMTransformer(model_name=self.model_name).model\n",
        "\n",
        "        config = AutoConfig.from_pretrained(self.model_name)\n",
        "\n",
        "        print(f\"Model max position embeddings: {config.max_position_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                #self.tokenizer.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "            print(f\"{model.__class__.__name__} instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        if torch.cuda.device_count() > 1 and self.device == \"cuda\":\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model)\n",
        "        model.eval()\n",
        "\n",
        "        print()\n",
        "        if self.INTRASENTENCE_LOAD_PATH:\n",
        "            state_dict = torch.load(self.INTRASENTENCE_LOAD_PATH)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        #pad_to_max_length = True if BATCH_SIZE > 1 else False\n",
        "\n",
        "        dataset = dataloader.IntrasentenceLoader(self.tokenizer, max_seq_length=self.max_seq_length,\n",
        "                                                 pad_to_max_length='max_length',\n",
        "                                                 input_file= self.input_file)\n",
        "\n",
        "        if self.intra_train_dataset is None or self.intra_test_dataset is None:\n",
        "          #reproducible split of dataset into train and test sets\n",
        "          gen = torch.Generator().manual_seed(41)\n",
        "          training_size = int(TRAINING_SET_SIZE_PERCENT * len(dataset))\n",
        "          test_size = len(dataset) - training_size\n",
        "          train_dataset, test_dataset = random_split(dataset, [training_size, test_size], generator=gen)\n",
        "          print(f\"First element of the [intrasentences] training set: {train_dataset[0]}\")\n",
        "          print(f\"First element of the [intrasentances] test set: {test_dataset[0]}\")\n",
        "          #save splitted datasets for future evaluation\n",
        "          self.intra_train_dataset = train_dataset\n",
        "          self.intra_test_dataset = test_dataset\n",
        "\n",
        "        #print(f'Sentence 0: {dataset.sentences[0]}')\n",
        "        data_loader = DataLoader(self.intra_test_dataset, batch_size=BATCH_SIZE)\n",
        "        word_probabilities = defaultdict(list)\n",
        "\n",
        "        print(\"Calculating intrasentence predictions...\")\n",
        "\n",
        "        # calculate the logits for each prediction\n",
        "        for sentence_id, next_token, input_ids, attention_mask, token_type_ids, sentence_label in tqdm(data_loader, total=len(data_loader)):\n",
        "            # start by converting everything to a tensor\n",
        "            if BATCH_SIZE == 1:\n",
        "              print(f\"Max attention mask value: {max(attention_mask)}\")\n",
        "              print(f\"Min attention mask value: {min(attention_mask)}\")\n",
        "              max_id = max(input_ids)\n",
        "              print(f\"Max input ID: {max_id}, Model vocab size: {self.tokenizer.vocab_size}\")\n",
        "\n",
        "            input_ids = input_ids.squeeze(1).to(self.device)\n",
        "            attention_mask = attention_mask.squeeze(1).to(self.device)\n",
        "            next_token = next_token.to(self.device) #token to predict\n",
        "            # else:  #in case of batch_size > 1\n",
        "            #   print(input_ids)\n",
        "            #   input_ids = torch.stack(input_ids).to(self.device).transpose(0, 1)\n",
        "            #   attention_mask = torch.stack(attention_mask).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "            #   next_token = next_token.to(self.device)\n",
        "            #   token_type_ids = torch.stack(token_type_ids).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "\n",
        "            mask_idxs = (input_ids == self.MASK_TOKEN_IDX)\n",
        "\n",
        "            # get the probabilities\n",
        "            output = model(input_ids, attention_mask=attention_mask)[0].softmax(dim=-1)\n",
        "\n",
        "            output = output[mask_idxs] #target only the masked positions\n",
        "            output = output.index_select(1, next_token).diag() #extract the probs of true tokens from the vocabulary dimension\n",
        "            for idx, item in enumerate(output):\n",
        "                word_probabilities[sentence_id[idx]].append((item.item(), sentence_label[idx]))\n",
        "\n",
        "        # now reconcile the probabilities into sentences\n",
        "        sentence_probabilties = []\n",
        "        for k, v in word_probabilities.items():\n",
        "            pred = {}\n",
        "            pred['id'] = k\n",
        "            #since we have n next tokens for the same sentance id, associated probs needs to be standarized\n",
        "            #in order to be compared with other labeled sentences' scores\n",
        "            v_scores = [v_k for v_k, _ in v]\n",
        "            # score = np.sum([np.log2(i) for i in v]) + np.log2(len(v))\n",
        "            score = np.mean(v_scores)\n",
        "            pred['score'] = score\n",
        "            \n",
        "            loss = self.loss_fn(torch.tensor(v_scores), torch.tensor([v_v for _, v_v in v]))\n",
        "            pred['loss'] = loss.item()\n",
        "\n",
        "            sentence_probabilties.append(pred)\n",
        "\n",
        "        return sentence_probabilties\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    def evaluate_intersentence(self, useAdapter=False):\n",
        "        model = AdaptedNSPTransformer().to(self.device)\n",
        "\n",
        "        config = AutoConfig.from_pretrained(self.model_name)\n",
        "\n",
        "        print(f\"Model max position embeddings: {config.max_position_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                #self.tokenizer.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "            print(f\"{model.__class__.__name__} instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        print(f\"Number of parameters: {self.count_parameters(model):,}\")\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "        if self.INTERSENTENCE_LOAD_PATH:\n",
        "            model.load_state_dict(torch.load(self.INTERSENTENCE_LOAD_PATH))\n",
        "\n",
        "        model.eval()\n",
        "        dataset = IntersentenceDataset(self.tokenizer)\n",
        "\n",
        "        if self.inter_train_dataset is None or self.inter_test_dataset is None:\n",
        "          #reproducible split of dataset into train and test sets\n",
        "          gen = torch.Generator().manual_seed(42)\n",
        "          training_size = int(0.85 * len(dataset))\n",
        "          test_size = len(dataset) - training_size\n",
        "          train_dataset, test_dataset = random_split(dataset, [training_size, test_size], generator=gen)\n",
        "          print(f\"First element of the [intersentences] training set: {train_dataset[0]}\")\n",
        "          print(f\"First element of the [intersentances] test set: {test_dataset[0]}\")\n",
        "          #save splitted datasets for future evaluation\n",
        "          self.inter_train_dataset = train_dataset\n",
        "          self.inter_test_dataset = test_dataset\n",
        "\n",
        "        #print(f'Sentence 0: {dataset.sentences[0]}')\n",
        "        data_loader = DataLoader(self.inter_test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        print(\"Calculating intersentence predictions...\")\n",
        "        if NO_CUDA:\n",
        "            n_cpus = cpu_count()\n",
        "            print(f\"Using {n_cpus} cpus!\")\n",
        "            predictions = Parallel(n_jobs=n_cpus, backend=\"multiprocessing\")(delayed(process_job)(\n",
        "                batch, model, self.model_name) for batch in tqdm(data_loader, total=len(data_loader)))\n",
        "        else:\n",
        "            predictions = []\n",
        "\n",
        "            for batch_num, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "                input_ids, token_type_ids, attention_mask, sentence_id, sentence_label = batch\n",
        "                input_ids = input_ids.squeeze(1).to(self.device)\n",
        "                attention_mask = attention_mask.squeeze(1).to(self.device)\n",
        "                #token_type_ids = token_type_ids.to(self.device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                if hasattr(outputs, \"logits\"):\n",
        "                    outputs = outputs.logits\n",
        "                else:\n",
        "                    outputs = outputs[0]\n",
        "                outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                for idx in range(input_ids.shape[0]):\n",
        "                    probabilities = {}\n",
        "                    probabilities['id'] = sentence_id[idx]\n",
        "                    #probability of the second sentence to be \"next\" to the first one, [idx, 1] corresponds to the positive class\n",
        "                    probabilities['score'] = outputs[idx, 1].item()\n",
        "\n",
        "                    loss = self.loss_fn(torch.tensor(outputs[idx, 1].item()), torch.tensor(sentence_label[idx]))\n",
        "                    probabilities['loss'] = loss.item()\n",
        "\n",
        "                    predictions.append(probabilities)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self):\n",
        "        predictions = {}\n",
        "        if not self.SKIP_INTERSENTENCE:\n",
        "            intersentence_predictions = self.evaluate_intersentence()\n",
        "            predictions['intersentence'] = intersentence_predictions\n",
        "\n",
        "        if not self.SKIP_INTRASENTENCE:\n",
        "            intrasentence_predictions = self.evaluate_intrasentence()\n",
        "            predictions['intrasentence'] = intrasentence_predictions\n",
        "        return predictions\n",
        "\n",
        "\n",
        "def process_job(batch, model, pretrained_class):\n",
        "    input_ids, token_type_ids, sentence_id = batch\n",
        "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "    if type(outputs) == tuple:\n",
        "        outputs = outputs[0]\n",
        "    outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    pid = sentence_id[0]\n",
        "    #probability of the second sentence to be \"next\" to the first one, [idx, 1] corresponds to the positive class\n",
        "    pscore = outputs[0, 1].item()\n",
        "    return (pid, pscore)\n",
        "\n",
        "pretrained_model_name=\"roberta-base\"\n",
        "evaluator = SentenceEvaluator(input_file=INPUT_FILE, model_name=pretrained_model_name)\n",
        "results = evaluator.evaluate()\n",
        "results\n",
        "\n",
        "#save evaluation predictions\n",
        "output_file = f\"{pretrained_model_name}_{OUTPUT_FILE_PRETRAINED}\"\n",
        "output_file = os.path.join(OUTPUT_DIR, output_file)\n",
        "with open(output_file, \"w+\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mk4waGZPbgDp",
        "outputId": "906c8da5-078d-4a9e-87a0-92748e744886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data/stereo_dataset.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n",
            "Underlying model moved to cuda.\n",
            "AdaptedNSPTransformer instance moved to GPU.\n",
            "Number of parameters: 124,647,170\n",
            "Let's use 1 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abudo\\source\\vscode_projects\\MindTheGap\\MindTheGap\\MindTheGap\\intersentence_loader.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(input_ids)\n",
            "c:\\Users\\abudo\\source\\vscode_projects\\MindTheGap\\MindTheGap\\MindTheGap\\intersentence_loader.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length found: -inf\n",
            "First element of the [intersentences] training set: (tensor([[    0, 13584,  1623,  1364,    25,    10,   475,  2137,     4,     2,\n",
            "             2, 22649, 31720,    32,   747, 11216,     4,     2,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), '8f4fda0da82ef660dc2b5a315cccf8be')\n",
            "First element of the [intersentances] test set: (tensor([[    0,   133, 22629,  5508,    10,   333,     9, 10717,     4,     2,\n",
            "             2,  1213, 33767, 17867,   859,  9412,    23,   106,     8, 33615,\n",
            "           553,   106,   141,    49,   360,    58,   164,     4,     2,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'c56fb293d77f807579f0ffbd6a37ae42')\n",
            "Calculating intersentence predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 150/192 [00:09<00:02, 16.02it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pretrained_model_name=\u001b[33m\"\u001b[39m\u001b[33mroberta-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m evaluator = SentenceEvaluator(input_file=INPUT_FILE, model_name=pretrained_model_name)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m results\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#save evaluation predictions\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 231\u001b[39m, in \u001b[36mSentenceEvaluator.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    229\u001b[39m predictions = {}\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.SKIP_INTERSENTENCE:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     intersentence_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_intersentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     predictions[\u001b[33m'\u001b[39m\u001b[33mintersentence\u001b[39m\u001b[33m'\u001b[39m] = intersentence_predictions\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.SKIP_INTRASENTENCE:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 223\u001b[39m, in \u001b[36mSentenceEvaluator.evaluate_intersentence\u001b[39m\u001b[34m(self, useAdapter)\u001b[39m\n\u001b[32m    221\u001b[39m             probabilities[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m] = sentence_id[idx]\n\u001b[32m    222\u001b[39m             \u001b[38;5;66;03m#probability of the second sentence to be \"next\" to the first one, [idx, 1] corresponds to the positive class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m             probabilities[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m             predictions.append(probabilities)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
