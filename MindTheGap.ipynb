{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mind the gap\n",
        "\n",
        "This project aims to identify, measure, and mitigate social biases, such as gender, race, or profession-related stereotypes, in lightweight transformer models through hands-on fine-tuning and evaluation on targeted NLP tasks. More specifically, the project should implement a four-step methodology, defined as follows:\n",
        "\n",
        "1. Choose a lightweight pre-trained transformer model (e.g., DistilBERT, ALBERT, RoBERTa-base) suitable for local fine-tuning and evaluation.\n",
        "2. Evaluate the presence and extent of social bias (e.g., gender, racial, or occupational stereotypes) using dedicated benchmark datasets. Both quantitative metrics and qualitative outputs should be evaluated.\n",
        "3. Apply a bias mitigation technique, such as **fine-tuning on curated counter-stereotypical data**, integrating **adapter layers**, or employing **contrastive learning**, while keeping the solution computationally efficient and transparent.\n",
        "4. Re-assess the model using the same benchmark(s) to measure improvements. Students should compare pre- and post-intervention results, discuss trade-offs (e.g., performance vs. fairness), and visualize the impact of their approach.\n",
        "\n",
        "#### Dataset\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://github.com/moinnadeem/StereoSet). Nadeem, M., Bethke, A., & Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. *arXiv preprint arXiv:2004.09456*.\n",
        "- [StereoSet: Measuring stereotypical bias in pretrained language models](https://huggingface.co/datasets/McGill-NLP/stereoset). Hugging Face co."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZodFEpEh5vD",
        "outputId": "064af7a7-d811-43dc-c382-45fd8929557e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import json\n",
        "#to output plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import importlib\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# %cd /content/\n",
        "# !ls -R\n",
        "# repo_name = \"MindTheGap\"\n",
        "# #!rm -rf {repo_name}\n",
        "# if not os.path.exists(repo_name):\n",
        "#     print(f\"Directory {repo_name} does not exist, proceeding with clone.\")\n",
        "#     !git clone https://github.com/Abudo-S/MindTheGap.git\n",
        "\n",
        "# %cd MindTheGap\n",
        "\n",
        "import AdaptedMLMTransformer as AdaptedMLMTransformer_Module\n",
        "import intersentence_loader as intersentence_loader_Module\n",
        "import dataloader\n",
        "importlib.reload(AdaptedMLMTransformer_Module) # in case of updates\n",
        "importlib.reload(intersentence_loader_Module) # in case of updates\n",
        "importlib.reload(dataloader) # in case of updates\n",
        "from AdaptedMLMTransformer import AdaptedMLMTransformer\n",
        "from intersentence_loader import IntersentenceDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General observations:\n",
        "-The given dataset contains 2123 intersentence contexts and 2106 intrasentence contexts, each context has 3 sentences that are labeled as ```stereotype, anti-stereotype and unrelated``` with respect to the target bias type.\n",
        "-Intrasentences focus on biases at the word or sub-phrase level at position \"BLANK\". Meanwhile intersentences focus on biases within the relationship between the context and its associated sentences.\n",
        "\n",
        "##### Tasks:\n",
        "- Intrasentences are considered as a masked-language-modeling (MLM) task in which the model is given a sentence where a certain percentage of the tokens have been replaced with a special [MASK] token. The model's task is to predict the original words/sub-words of the masked tokens based on the surrounding context.\n",
        "- Intersentences are considered as a natural-language-inference (NLI) task in which the model is given a pair of sentences, a \"premise\" (context) and a \"hypothesis\" (one of the associated sentences) . Then the model classifies the relationship between them into one of three categories:\n",
        "\n",
        "    * Entailment: The hypothesis is logically derived from the premise.\n",
        "\n",
        "    * Contradiction: The hypothesis is logically inconsistent with the premise.\n",
        "\n",
        "    * Neutral: The two sentences are unrelated or their relationship cannot be determined.\n",
        "\n",
        "softmax => #[entailment_logit, contradiction_logit, neutral_logit], argmax will return 0, 1, or 2, which correspond to the index of the highest logit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLn3HuBpdezQ"
      },
      "outputs": [],
      "source": [
        "# original_init = AdaptedMLMTransformer_Module.AdaptedMLMTransformer.__init__\n",
        "\n",
        "# # Corrected debug_init signature - assuming only model_name is a required parameter\n",
        "# def debug_init(self, model_name=\"roberta-base\"):\n",
        "#     print(f\"Initializing AdaptedMLMTransformer with model_name: {model_name}\")\n",
        "#     # Pass only the parameters that original_init expects\n",
        "#     original_init(self, model_name=model_name)\n",
        "#     print(\"AdaptedMLMTransformer initialized.\")\n",
        "#     for name, param in self.named_parameters():\n",
        "#         print(f\"Layer: {name}, Device: {param.device}, Dtype: {param.dtype}\")\n",
        "\n",
        "# AdaptedMLMTransformer_Module.LightTAdaptedMLMTransformerransformerModel.__init__ = debug_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YEZqNR5tnUOv",
        "outputId": "c48924c1-6de3-4e7a-a5ab-cbd169904b1a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'BiasEvaluator' object has no attribute 'input_file'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 219\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (pid, pscore)\n\u001b[32m    218\u001b[39m pretrained_model_name=\u001b[33m\"\u001b[39m\u001b[33mroberta-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m evaluator = \u001b[43mBiasEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_intersentence\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m results = evaluator.evaluate()\n\u001b[32m    221\u001b[39m results\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mBiasEvaluator.__init__\u001b[39m\u001b[34m(self, input_file, model_name, intersentence_load_path, intrasentence_load_path, skip_intrasentence, skip_intersentence)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     13\u001b[39m              input_file=\u001b[33m\"\u001b[39m\u001b[33mdata/stereo_dataset.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m              model_name=\u001b[33m\"\u001b[39m\u001b[33mroberta-base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m              skip_intrasentence=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     18\u001b[39m              skip_intersentence=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m#self.dataloader = dataloader.StereoSet(os.path.abspath(INPUT_FILE))\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_file = input_file\n",
            "\u001b[31mAttributeError\u001b[39m: 'BiasEvaluator' object has no attribute 'input_file'"
          ]
        }
      ],
      "source": [
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "# os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
        "INPUT_FILE = \"data/stereo_dataset.json\" #\"/content/MindTheGap/data/stereo_dataset.json\"\n",
        "OUTPUT_FILE_PRETRAINED =\"predictions.json\"\n",
        "OUTPUT_FILE_FINETUNED =\"predictions.json\"\n",
        "OUTPUT_DIR = \"predictions/\"\n",
        "BATCH_SIZE = 5\n",
        "MAX_SEQ_LENGTH = 128\n",
        "NO_CUDA = False\n",
        "\n",
        "class BiasEvaluator():\n",
        "    def __init__(self,\n",
        "                 input_file=\"data/stereo_dataset.json\",\n",
        "                 model_name=\"roberta-base\",\n",
        "                 intersentence_load_path=None, \n",
        "                 intrasentence_load_path=None, \n",
        "                 skip_intrasentence=False,\n",
        "                 skip_intersentence=False):\n",
        "        print(f\"Loading {input_file}...\")\n",
        "\n",
        "        #self.dataloader = dataloader.StereoSet(os.path.abspath(input_file))\n",
        "        self.input_file = input_file\n",
        "        self.model_name = model_name\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "        self.SKIP_INTERSENTENCE = skip_intersentence\n",
        "        self.SKIP_INTRASENTENCE = skip_intrasentence\n",
        "        self.tokenizer = AdaptedMLMTransformer(model_name=self.model_name).tokenizer\n",
        "        self.device = \"cuda\" if not NO_CUDA else \"cpu\"\n",
        "\n",
        "        # to keep padding consistent with the other models -> improves LM score.\n",
        "        if self.tokenizer.__class__.__name__ == \"XLNetTokenizer\":\n",
        "            self.tokenizer.padding_side = \"right\"\n",
        "        self.MASK_TOKEN = self.tokenizer.mask_token\n",
        "\n",
        "        # Set this to be none if you don't want to batch items together!\n",
        "        self.max_seq_length = None if BATCH_SIZE == 1 else MAX_SEQ_LENGTH\n",
        "\n",
        "        self.MASK_TOKEN_IDX = self.tokenizer.encode(\n",
        "            self.MASK_TOKEN, add_special_tokens=False)\n",
        "        assert len(self.MASK_TOKEN_IDX) == 1\n",
        "        self.MASK_TOKEN_IDX = self.MASK_TOKEN_IDX[0]\n",
        "\n",
        "    def evaluate_intrasentence(self):\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"Using pretrained class: {self.model_name}\")\n",
        "\n",
        "        model = AdaptedMLMTransformer(model_name=self.model_name).model\n",
        "\n",
        "        config = AutoConfig.from_pretrained(self.model_name)\n",
        "\n",
        "        print(f\"Model max position embeddings: {config.max_position_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                model.tokenizer.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "            print(f\"{model.__class__.__name__} instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        if torch.cuda.device_count() > 1 and self.device == \"cuda\":\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model)\n",
        "        model.eval()\n",
        "\n",
        "        print()\n",
        "        if self.INTRASENTENCE_LOAD_PATH:\n",
        "            state_dict = torch.load(self.INTRASENTENCE_LOAD_PATH)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        #pad_to_max_length = True if BATCH_SIZE > 1 else False\n",
        "\n",
        "        #11017 intrasentences\n",
        "        dataset = dataloader.IntrasentenceLoader(self.tokenizer, max_seq_length=self.max_seq_length,\n",
        "                                                 pad_to_max_length='max_length',\n",
        "                                                 input_file= self.input_file)\n",
        "        \n",
        "        #reproducible split of dataset into train and test sets\n",
        "        gen = torch.Generator().manual_seed(42)\n",
        "        training_size = int(0.85 * len(dataset))\n",
        "        test_size = len(dataset) - training_size\n",
        "        train_dataset, test_dataset = random_split(dataset, [training_size, test_size], generator=gen)\n",
        "        print(f\"First element of the training set: {train_dataset[0]}\")\n",
        "        print(f\"First element of the test set: {test_dataset[0]}\")  \n",
        "\n",
        "        print(f'Sentence 0: {dataset.sentences[0]}')\n",
        "        training_set_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "        word_probabilities = defaultdict(list)\n",
        "\n",
        "        # calculate the logits for each prediction\n",
        "        for sentence_id, next_token, input_ids, attention_mask, token_type_ids in tqdm(training_set_loader, total=len(training_set_loader)):\n",
        "            # start by converting everything to a tensor\n",
        "            if BATCH_SIZE == 1:\n",
        "              print(f\"Max attention mask value: {max(attention_mask)}\")\n",
        "              print(f\"Min attention mask value: {min(attention_mask)}\")\n",
        "              max_id = max(input_ids)\n",
        "              print(f\"Max input ID: {max_id}, Model vocab size: {self.tokenizer.vocab_size}\")\n",
        "\n",
        "            input_ids = input_ids.squeeze(1).to(self.device)\n",
        "            attention_mask = attention_mask.squeeze(1).to(self.device)\n",
        "            next_token = next_token.to(self.device)\n",
        "            # else:  #in case of batch_size > 1\n",
        "            #   print(input_ids)\n",
        "            #   input_ids = torch.stack(input_ids).to(self.device).transpose(0, 1)\n",
        "            #   attention_mask = torch.stack(attention_mask).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "            #   next_token = next_token.to(self.device)\n",
        "            #   token_type_ids = torch.stack(token_type_ids).to(\n",
        "            #     self.device).transpose(0, 1)\n",
        "\n",
        "            mask_idxs = (input_ids == self.MASK_TOKEN_IDX)\n",
        "\n",
        "            # get the probabilities\n",
        "            output = model(input_ids, attention_mask=attention_mask)[0].softmax(dim=-1)\n",
        "\n",
        "            output = output[mask_idxs] #target only the masked positions\n",
        "            output = output.index_select(1, next_token).diag() #extract the probs of true tokens from the vocabulary dimension\n",
        "            for idx, item in enumerate(output):\n",
        "                word_probabilities[sentence_id[idx]].append(item.item())\n",
        "\n",
        "        # now reconcile the probabilities into sentences\n",
        "        sentence_probabilties = []\n",
        "        for k, v in word_probabilities.items():\n",
        "            pred = {}\n",
        "            pred['id'] = k\n",
        "            #since we have n next tokens for the same sentance id, associated probs needs to be standarized\n",
        "            #in order to be compared to other labeled sentences' scores\n",
        "            # score = np.sum([np.log2(i) for i in v]) + np.log2(len(v))\n",
        "            score = np.mean(v) \n",
        "            pred['score'] = score\n",
        "\n",
        "            sentence_probabilties.append(pred)\n",
        "\n",
        "        return sentence_probabilties\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    def evaluate_intersentence(self):\n",
        "        print()\n",
        "        model = AdaptedMLMTransformer().to(self.device)\n",
        "\n",
        "        print(f\"Number of parameters: {self.count_parameters(model):,}\")\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "        if self.INTERSENTENCE_LOAD_PATH:\n",
        "            model.load_state_dict(torch.load(self.INTERSENTENCE_LOAD_PATH))\n",
        "\n",
        "        model.eval()\n",
        "        dataset = IntersentenceDataset(self.tokenizer)\n",
        "        # TODO: test this on larger batch sizes.\n",
        "        #assert args.batch_size == 1\n",
        "        dataloader = DataLoader(dataset, shuffle=True, num_workers=0)\n",
        "\n",
        "        if not self.cuda:\n",
        "            n_cpus = cpu_count()\n",
        "            print(f\"Using {n_cpus} cpus!\")\n",
        "            predictions = Parallel(n_jobs=n_cpus, backend=\"multiprocessing\")(delayed(process_job)(\n",
        "                batch, model, self.model_name) for batch in tqdm(dataloader, total=len(dataloader)))\n",
        "        else:\n",
        "            predictions = []\n",
        "\n",
        "            for batch_num, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "                input_ids, token_type_ids, attention_mask, sentence_id = batch\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                token_type_ids = token_type_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "                if type(outputs) == tuple:\n",
        "                    outputs = outputs[0]\n",
        "                outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                for idx in range(input_ids.shape[0]):\n",
        "                    probabilities = {}\n",
        "                    probabilities['id'] = sentence_id[idx]\n",
        "                    if \"bert\" == self.model_name[:4] or \"roberta-base\" == self.model_name:\n",
        "                        probabilities['score'] = outputs[idx, 0].item()\n",
        "                    else:\n",
        "                        probabilities['score'] = outputs[idx, 1].item()\n",
        "                    predictions.append(probabilities)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self):\n",
        "        bias = {}\n",
        "        if not self.SKIP_INTERSENTENCE:\n",
        "            intersentence_bias = self.evaluate_intersentence()\n",
        "            bias['intersentence'] = intersentence_bias\n",
        "\n",
        "        if not self.SKIP_INTRASENTENCE:\n",
        "            intrasentence_bias = self.evaluate_intrasentence()\n",
        "            bias['intrasentence'] = intrasentence_bias\n",
        "        return bias\n",
        "\n",
        "\n",
        "def process_job(batch, model, pretrained_class):\n",
        "    input_ids, token_type_ids, sentence_id = batch\n",
        "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "    if type(outputs) == tuple:\n",
        "        outputs = outputs[0]\n",
        "    outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    pid = sentence_id[0]\n",
        "    # if \"bert\"==self.PRETRAINED_CLASS[:4]:\n",
        "    if \"bert\" in pretrained_class:\n",
        "        pscore = outputs[0, 0].item()\n",
        "    else:\n",
        "        pscore = outputs[0, 1].item()\n",
        "    return (pid, pscore)\n",
        "\n",
        "\n",
        "pretrained_model_name=\"roberta-base\"\n",
        "evaluator = BiasEvaluator(input_file=INPUT_FILE, model_name=pretrained_model_name, skip_intersentence=True)\n",
        "results = evaluator.evaluate()\n",
        "results\n",
        "\n",
        "output_file = f\"{pretrained_model_name}_{OUTPUT_FILE_PRETRAINED}\"\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, output_file)\n",
        "with open(output_file, \"w+\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
