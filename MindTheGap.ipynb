{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZodFEpEh5vD",
        "outputId": "0e186db7-eaeb-4591-dcf3-bac09fc7b2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            ".:\n",
            "MindTheGap  sample_data\n",
            "\n",
            "./MindTheGap:\n",
            "data\t       intersentence_loader.py\t MindTheGap.ipynb  README.md\n",
            "dataloader.py  LightTransformerModel.py  __pycache__\t   requirements.txt\n",
            "\n",
            "./MindTheGap/data:\n",
            "stereo_dataset.json  test_terms.txt\n",
            "\n",
            "./MindTheGap/__pycache__:\n",
            "dataloader.cpython-312.pyc\t      LightTransformerModel.cpython-312.pyc\n",
            "intersentence_loader.cpython-312.pyc\n",
            "\n",
            "./sample_data:\n",
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n",
            "/content/MindTheGap\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "#to output plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import importlib\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "\n",
        "%cd /content/\n",
        "!ls -R\n",
        "repo_name = \"MindTheGap\"\n",
        "#!rm -rf {repo_name}\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Directory {repo_name} does not exist, proceeding with clone.\")\n",
        "    !git clone https://github.com/Abudo-S/MindTheGap.git\n",
        "\n",
        "%cd MindTheGap\n",
        "\n",
        "\n",
        "import LightTransformerModel as LightTransformerModel_Module\n",
        "import intersentence_loader as intersentence_loader_Module\n",
        "import dataloader\n",
        "importlib.reload(LightTransformerModel_Module) # in case of updates\n",
        "importlib.reload(intersentence_loader_Module) # in case of updates\n",
        "importlib.reload(dataloader) # in case of updates\n",
        "from LightTransformerModel import LightTransformerModel\n",
        "from intersentence_loader import IntersentenceDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "class BiasEvaluator():\n",
        "    def __init__(self, pretrained_class=\"roberta-base\", no_cuda=False,\n",
        "                 input_file=\"/content/MindTheGap/data/stereo_dataset.json\", tokenizer=\"roberta-base\",\n",
        "                 intersentence_load_path=None, intrasentence_load_path=None, skip_intrasentence=False,\n",
        "                 skip_intersentence=False, batch_size=1, max_seq_length=128,\n",
        "                 output_dir=\"predictions/\", output_file=\"predictions.json\"):\n",
        "        print(f\"Loading {input_file}...\")\n",
        "        filename = os.path.abspath(input_file)\n",
        "        self.dataloader = dataloader.StereoSet(filename)\n",
        "        self.cuda = not no_cuda\n",
        "        self.device = \"cuda\" if self.cuda else \"cpu\"\n",
        "\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "        self.SKIP_INTERSENTENCE = skip_intersentence\n",
        "        self.SKIP_INTRASENTENCE = skip_intrasentence\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "\n",
        "        self.PRETRAINED_CLASS = pretrained_class\n",
        "        self.TOKENIZER = tokenizer\n",
        "        self.tokenizer = LightTransformerModel(model_name=self.PRETRAINED_CLASS).tokenizer\n",
        "\n",
        "        # to keep padding consistent with the other models -> improves LM score.\n",
        "        if self.tokenizer.__class__.__name__ == \"XLNetTokenizer\":\n",
        "            self.tokenizer.padding_side = \"right\"\n",
        "        self.MASK_TOKEN = self.tokenizer.mask_token\n",
        "\n",
        "        # Set this to be none if you don't want to batch items together!\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_length = None if self.batch_size == 1 else max_seq_length\n",
        "\n",
        "        self.MASK_TOKEN_IDX = self.tokenizer.encode(\n",
        "            self.MASK_TOKEN, add_special_tokens=False)\n",
        "        assert len(self.MASK_TOKEN_IDX) == 1\n",
        "        self.MASK_TOKEN_IDX = self.MASK_TOKEN_IDX[0]\n",
        "\n",
        "    def evaluate_intrasentence(self):\n",
        "        model = LightTransformerModel(model_name=self.PRETRAINED_CLASS).to(self.device)\n",
        "\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model)\n",
        "        model.eval()\n",
        "\n",
        "        print()\n",
        "        if self.INTRASENTENCE_LOAD_PATH:\n",
        "            state_dict = torch.load(self.INTRASENTENCE_LOAD_PATH)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        pad_to_max_length = True if self.batch_size > 1 else False\n",
        "        dataset = dataloader.IntrasentenceLoader(self.tokenizer, max_seq_length=self.max_seq_length,\n",
        "                                                 pad_to_max_length=pad_to_max_length,\n",
        "                                                 input_file= \"/content/MindTheGap/data/stereo_dataset.json\",)\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "        word_probabilities = defaultdict(list)\n",
        "\n",
        "        # calculate the logits for each prediction\n",
        "        for sentence_id, next_token, input_ids, attention_mask, token_type_ids in tqdm(loader, total=len(loader)):\n",
        "            # start by converting everything to a tensor\n",
        "            input_ids = torch.stack(input_ids).to(self.device).transpose(0, 1)\n",
        "            attention_mask = torch.stack(attention_mask).to(\n",
        "                self.device).transpose(0, 1)\n",
        "            next_token = next_token.to(self.device)\n",
        "            token_type_ids = torch.stack(token_type_ids).to(\n",
        "                self.device).transpose(0, 1)\n",
        "\n",
        "            mask_idxs = (input_ids == self.MASK_TOKEN_IDX)\n",
        "\n",
        "            # get the probabilities\n",
        "            output = model(input_ids, attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids)[0].softmax(dim=-1)\n",
        "\n",
        "            output = output[mask_idxs]\n",
        "            output = output.index_select(1, next_token).diag()\n",
        "            for idx, item in enumerate(output):\n",
        "                word_probabilities[sentence_id[idx]].append(item.item())\n",
        "\n",
        "        # now reconcile the probabilities into sentences\n",
        "        sentence_probabilties = []\n",
        "        for k, v in word_probabilities.items():\n",
        "            pred = {}\n",
        "            pred['id'] = k\n",
        "            # score = np.sum([np.log2(i) for i in v]) + np.log2(len(v))\n",
        "            score = np.mean(v)\n",
        "            pred['score'] = score\n",
        "            sentence_probabilties.append(pred)\n",
        "\n",
        "        return sentence_probabilties\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    def evaluate_intersentence(self):\n",
        "        print()\n",
        "        model = LightTransformerModel().to(self.device)\n",
        "\n",
        "        print(f\"Number of parameters: {self.count_parameters(model):,}\")\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "        if self.INTERSENTENCE_LOAD_PATH:\n",
        "            model.load_state_dict(torch.load(self.INTERSENTENCE_LOAD_PATH))\n",
        "\n",
        "        model.eval()\n",
        "        dataset = IntersentenceDataset(self.tokenizer)\n",
        "        # TODO: test this on larger batch sizes.\n",
        "        #assert args.batch_size == 1\n",
        "        dataloader = DataLoader(dataset, shuffle=True, num_workers=0)\n",
        "\n",
        "        if not self.cuda:\n",
        "            n_cpus = cpu_count()\n",
        "            print(f\"Using {n_cpus} cpus!\")\n",
        "            predictions = Parallel(n_jobs=n_cpus, backend=\"multiprocessing\")(delayed(process_job)(\n",
        "                batch, model, self.PRETRAINED_CLASS) for batch in tqdm(dataloader, total=len(dataloader)))\n",
        "        else:\n",
        "            predictions = []\n",
        "\n",
        "            for batch_num, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "                input_ids, token_type_ids, attention_mask, sentence_id = batch\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                token_type_ids = token_type_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "                if type(outputs) == tuple:\n",
        "                    outputs = outputs[0]\n",
        "                outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                for idx in range(input_ids.shape[0]):\n",
        "                    probabilities = {}\n",
        "                    probabilities['id'] = sentence_id[idx]\n",
        "                    if \"bert\" == self.PRETRAINED_CLASS[:4] or \"roberta-base\" == self.PRETRAINED_CLASS:\n",
        "                        probabilities['score'] = outputs[idx, 0].item()\n",
        "                    else:\n",
        "                        probabilities['score'] = outputs[idx, 1].item()\n",
        "                    predictions.append(probabilities)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self):\n",
        "        bias = {}\n",
        "        if not self.SKIP_INTERSENTENCE:\n",
        "            intersentence_bias = self.evaluate_intersentence()\n",
        "            bias['intersentence'] = intersentence_bias\n",
        "\n",
        "        if not self.SKIP_INTRASENTENCE:\n",
        "            intrasentence_bias = self.evaluate_intrasentence()\n",
        "            bias['intrasentence'] = intrasentence_bias\n",
        "        return bias\n",
        "\n",
        "\n",
        "def process_job(batch, model, pretrained_class):\n",
        "    input_ids, token_type_ids, sentence_id = batch\n",
        "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "    if type(outputs) == tuple:\n",
        "        outputs = outputs[0]\n",
        "    outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    pid = sentence_id[0]\n",
        "    # if \"bert\"==self.PRETRAINED_CLASS[:4]:\n",
        "    if \"bert\" in pretrained_class:\n",
        "        pscore = outputs[0, 0].item()\n",
        "    else:\n",
        "        pscore = outputs[0, 1].item()\n",
        "    return (pid, pscore)\n",
        "\n",
        "\n",
        "evaluator = BiasEvaluator(pretrained_class=\"roberta-base\", skip_intersentence=True)\n",
        "results = evaluator.evaluate()\n",
        "results\n",
        "# if args.output_file is not None:\n",
        "#     output_file = args.output_file\n",
        "# else:\n",
        "#     output_file = f\"predictions_{args.pretrained_class}_{args.intersentence_model}_{args.intrasentence_model}.json\"\n",
        "\n",
        "# output_file = os.path.join(args.output_dir, output_file)\n",
        "# with open(output_file, \"w+\") as f:\n",
        "#     json.dump(results, f, indent=2)\n"
      ],
      "metadata": {
        "id": "YEZqNR5tnUOv",
        "outputId": "6463952f-3c90-4444-e791-4038c1035038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/MindTheGap/data/stereo_dataset.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4230276162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiasEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"roberta-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_intersentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# if args.output_file is not None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4230276162.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKIP_INTRASENTENCE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mintrasentence_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_intrasentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intrasentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrasentence_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4230276162.py\u001b[0m in \u001b[0;36mevaluate_intrasentence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_intrasentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightTransformerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_CLASS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}