{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0ZodFEpEh5vD",
        "outputId": "85e582c2-210f-4556-af7f-0280dadc6af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            ".:\n",
            "sample_data\n",
            "\n",
            "./sample_data:\n",
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n",
            "Directory MindTheGap does not exist, proceeding with clone.\n",
            "Cloning into 'MindTheGap'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 49 (delta 22), reused 32 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 1.31 MiB | 4.14 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "/content/MindTheGap\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "#to output plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import importlib\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "!ls -R\n",
        "repo_name = \"MindTheGap\"\n",
        "#!rm -rf {repo_name}\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Directory {repo_name} does not exist, proceeding with clone.\")\n",
        "    !git clone https://github.com/Abudo-S/MindTheGap.git\n",
        "\n",
        "%cd MindTheGap\n",
        "\n",
        "import LightTransformerModel as LightTransformerModel_Module\n",
        "import intersentence_loader as intersentence_loader_Module\n",
        "import dataloader\n",
        "importlib.reload(LightTransformerModel_Module) # in case of updates\n",
        "importlib.reload(intersentence_loader_Module) # in case of updates\n",
        "importlib.reload(dataloader) # in case of updates\n",
        "from LightTransformerModel import LightTransformerModel\n",
        "from intersentence_loader import IntersentenceDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# original_init = LightTransformerModel_Module.LightTransformerModel.__init__\n",
        "\n",
        "# # Corrected debug_init signature - assuming only model_name is a required parameter\n",
        "# def debug_init(self, model_name=\"roberta-base\"):\n",
        "#     print(f\"Initializing LightTransformerModel with model_name: {model_name}\")\n",
        "#     # Pass only the parameters that original_init expects\n",
        "#     original_init(self, model_name=model_name)\n",
        "#     print(\"LightTransformerModel initialized.\")\n",
        "#     for name, param in self.named_parameters():\n",
        "#         print(f\"Layer: {name}, Device: {param.device}, Dtype: {param.dtype}\")\n",
        "\n",
        "# LightTransformerModel_Module.LightTransformerModel.__init__ = debug_init"
      ],
      "metadata": {
        "id": "pLn3HuBpdezQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
        "\n",
        "class BiasEvaluator():\n",
        "    def __init__(self, pretrained_class=\"roberta-base\", no_cuda=False,\n",
        "                 input_file=\"/content/MindTheGap/data/stereo_dataset.json\", tokenizer=\"roberta-base\",\n",
        "                 intersentence_load_path=None, intrasentence_load_path=None, skip_intrasentence=False,\n",
        "                 skip_intersentence=False, batch_size=1, max_seq_length=128,\n",
        "                 output_dir=\"predictions/\", output_file=\"predictions.json\"):\n",
        "        print(f\"Loading {input_file}...\")\n",
        "        filename = os.path.abspath(input_file)\n",
        "        self.dataloader = dataloader.StereoSet(filename)\n",
        "        self.cuda = not no_cuda\n",
        "        self.device = \"cuda\" if self.cuda else \"cpu\"\n",
        "\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "        self.SKIP_INTERSENTENCE = skip_intersentence\n",
        "        self.SKIP_INTRASENTENCE = skip_intrasentence\n",
        "        self.INTRASENTENCE_LOAD_PATH = intrasentence_load_path\n",
        "        self.INTERSENTENCE_LOAD_PATH = intersentence_load_path\n",
        "\n",
        "        self.PRETRAINED_CLASS = pretrained_class\n",
        "        self.TOKENIZER = tokenizer\n",
        "        self.tokenizer = LightTransformerModel(model_name=self.PRETRAINED_CLASS).tokenizer\n",
        "\n",
        "        # to keep padding consistent with the other models -> improves LM score.\n",
        "        if self.tokenizer.__class__.__name__ == \"XLNetTokenizer\":\n",
        "            self.tokenizer.padding_side = \"right\"\n",
        "        self.MASK_TOKEN = self.tokenizer.mask_token\n",
        "\n",
        "        # Set this to be none if you don't want to batch items together!\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_length = None if self.batch_size == 1 else max_seq_length\n",
        "\n",
        "        self.MASK_TOKEN_IDX = self.tokenizer.encode(\n",
        "            self.MASK_TOKEN, add_special_tokens=False)\n",
        "        assert len(self.MASK_TOKEN_IDX) == 1\n",
        "        self.MASK_TOKEN_IDX = self.MASK_TOKEN_IDX[0]\n",
        "\n",
        "    def evaluate_intrasentence(self):\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"Using pretrained class: {self.PRETRAINED_CLASS}\")\n",
        "\n",
        "        model = LightTransformerModel(model_name=self.PRETRAINED_CLASS).model\n",
        "        from transformers import AutoConfig\n",
        "\n",
        "        # Load the configuration of your model (e.g., 'roberta-base')\n",
        "        model_name = \"roberta-base\"\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "        max_pos_embeddings = config.max_position_embeddings\n",
        "        print(f\"Model max position embeddings: {max_pos_embeddings}\")\n",
        "        if torch.cuda.is_available() and self.device == \"cuda\":\n",
        "            print(\"Moving model to GPU...\")\n",
        "            model.to(self.device)\n",
        "            # Explicitly move the underlying model to the device as well\n",
        "            if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
        "                model.model.to(self.device)\n",
        "                print(f\"Underlying model moved to {self.device}.\")\n",
        "                # Check the device of a parameter in the underlying model\n",
        "                for name, param in model.model.named_parameters():\n",
        "                    print(f\"Underlying model parameter: {name}, Device: {param.device}\")\n",
        "                    break # Just check one parameter\n",
        "            print(\"LightTransformerModel instance moved to GPU.\")\n",
        "        else:\n",
        "            print(\"CUDA is not available or device is not set to cuda, using CPU.\")\n",
        "\n",
        "\n",
        "        if torch.cuda.device_count() > 1 and self.device == \"cuda\":\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model)\n",
        "        model.eval()\n",
        "\n",
        "        print()\n",
        "        if self.INTRASENTENCE_LOAD_PATH:\n",
        "            state_dict = torch.load(self.INTRASENTENCE_LOAD_PATH)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        pad_to_max_length = True if self.batch_size > 1 else False\n",
        "\n",
        "        dataset = dataloader.IntrasentenceLoader(self.tokenizer, max_seq_length=self.max_seq_length,\n",
        "                                                 pad_to_max_length='max_length',\n",
        "                                                 input_file= \"/content/MindTheGap/data/stereo_dataset.json\")\n",
        "        print(f'sentence 0: {dataset.sentences[0]}')\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "        word_probabilities = defaultdict(list)\n",
        "\n",
        "        # calculate the logits for each prediction\n",
        "        for sentence_id, next_token, input_ids, attention_mask, token_type_ids in tqdm(loader, total=len(loader)):\n",
        "            print(f\"Max attention mask value: {max(t.max() for t in attention_mask)}\")\n",
        "            print(f\"Min attention mask value: {min(t.min() for t in attention_mask)}\")\n",
        "            max_id = max(t.max() for t in input_ids)\n",
        "            print(f\"Max input ID: {max_id}, Model vocab size: {self.tokenizer.vocab_size}\")\n",
        "            # start by converting everything to a tensor\n",
        "            input_ids = torch.stack(input_ids).to(self.device).transpose(0, 1)\n",
        "            attention_mask = torch.stack(attention_mask).to(\n",
        "                self.device).transpose(0, 1)\n",
        "            next_token = next_token.to(self.device)\n",
        "            token_type_ids = torch.stack(token_type_ids).to(\n",
        "                self.device).transpose(0, 1)\n",
        "\n",
        "            mask_idxs = (input_ids == self.MASK_TOKEN_IDX)\n",
        "\n",
        "            # get the probabilities\n",
        "            output = model(input_ids, attention_mask=attention_mask)[0].softmax(dim=-1)\n",
        "\n",
        "            output = output[mask_idxs]\n",
        "            output = output.index_select(1, next_token).diag()\n",
        "            for idx, item in enumerate(output):\n",
        "                word_probabilities[sentence_id[idx]].append(item.item())\n",
        "\n",
        "        # now reconcile the probabilities into sentences\n",
        "        sentence_probabilties = []\n",
        "        for k, v in word_probabilities.items():\n",
        "            pred = {}\n",
        "            pred['id'] = k\n",
        "            # score = np.sum([np.log2(i) for i in v]) + np.log2(len(v))\n",
        "            score = np.mean(v)\n",
        "            pred['score'] = score\n",
        "            sentence_probabilties.append(pred)\n",
        "\n",
        "        return sentence_probabilties\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    def evaluate_intersentence(self):\n",
        "        print()\n",
        "        model = LightTransformerModel().to(self.device)\n",
        "\n",
        "        print(f\"Number of parameters: {self.count_parameters(model):,}\")\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "        if self.INTERSENTENCE_LOAD_PATH:\n",
        "            model.load_state_dict(torch.load(self.INTERSENTENCE_LOAD_PATH))\n",
        "\n",
        "        model.eval()\n",
        "        dataset = IntersentenceDataset(self.tokenizer)\n",
        "        # TODO: test this on larger batch sizes.\n",
        "        #assert args.batch_size == 1\n",
        "        dataloader = DataLoader(dataset, shuffle=True, num_workers=0)\n",
        "\n",
        "        if not self.cuda:\n",
        "            n_cpus = cpu_count()\n",
        "            print(f\"Using {n_cpus} cpus!\")\n",
        "            predictions = Parallel(n_jobs=n_cpus, backend=\"multiprocessing\")(delayed(process_job)(\n",
        "                batch, model, self.PRETRAINED_CLASS) for batch in tqdm(dataloader, total=len(dataloader)))\n",
        "        else:\n",
        "            predictions = []\n",
        "\n",
        "            for batch_num, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "                input_ids, token_type_ids, attention_mask, sentence_id = batch\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                token_type_ids = token_type_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "                if type(outputs) == tuple:\n",
        "                    outputs = outputs[0]\n",
        "                outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                for idx in range(input_ids.shape[0]):\n",
        "                    probabilities = {}\n",
        "                    probabilities['id'] = sentence_id[idx]\n",
        "                    if \"bert\" == self.PRETRAINED_CLASS[:4] or \"roberta-base\" == self.PRETRAINED_CLASS:\n",
        "                        probabilities['score'] = outputs[idx, 0].item()\n",
        "                    else:\n",
        "                        probabilities['score'] = outputs[idx, 1].item()\n",
        "                    predictions.append(probabilities)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self):\n",
        "        bias = {}\n",
        "        if not self.SKIP_INTERSENTENCE:\n",
        "            intersentence_bias = self.evaluate_intersentence()\n",
        "            bias['intersentence'] = intersentence_bias\n",
        "\n",
        "        if not self.SKIP_INTRASENTENCE:\n",
        "            intrasentence_bias = self.evaluate_intrasentence()\n",
        "            bias['intrasentence'] = intrasentence_bias\n",
        "        return bias\n",
        "\n",
        "\n",
        "def process_job(batch, model, pretrained_class):\n",
        "    input_ids, token_type_ids, sentence_id = batch\n",
        "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
        "    if type(outputs) == tuple:\n",
        "        outputs = outputs[0]\n",
        "    outputs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    pid = sentence_id[0]\n",
        "    # if \"bert\"==self.PRETRAINED_CLASS[:4]:\n",
        "    if \"bert\" in pretrained_class:\n",
        "        pscore = outputs[0, 0].item()\n",
        "    else:\n",
        "        pscore = outputs[0, 1].item()\n",
        "    return (pid, pscore)\n",
        "\n",
        "\n",
        "evaluator = BiasEvaluator(pretrained_class=\"roberta-base\", skip_intersentence=True)\n",
        "results = evaluator.evaluate()\n",
        "results\n",
        "# if args.output_file is not None:\n",
        "#     output_file = args.output_file\n",
        "# else:\n",
        "#     output_file = f\"predictions_{args.pretrained_class}_{args.intersentence_model}_{args.intrasentence_model}.json\"\n",
        "\n",
        "# output_file = os.path.join(args.output_dir, output_file)\n",
        "# with open(output_file, \"w+\") as f:\n",
        "#     json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "YEZqNR5tnUOv",
        "outputId": "2a4f0887-c878-4013-f03f-d89b5f23acfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/MindTheGap/data/stereo_dataset.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using pretrained class: roberta-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model max position embeddings: 514\n",
            "Moving model to GPU...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1901354366.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiasEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"roberta-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_intersentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;31m# if args.output_file is not None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1901354366.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKIP_INTRASENTENCE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mintrasentence_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_intrasentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intrasentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintrasentence_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1901354366.py\u001b[0m in \u001b[0;36mevaluate_intrasentence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Moving model to GPU...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Explicitly move the underlying model to the device as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4457\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4458\u001b[0m                 )\n\u001b[0;32m-> 4459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}